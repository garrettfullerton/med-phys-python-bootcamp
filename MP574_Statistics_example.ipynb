{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code example: MP574 - Basic Statistics\n",
    "\n",
    "*Disclamer*: This notebook more so covers running basic statistics in python. In 2023, the MP574 class was heavy on statistics with an end learning goal of learning the fundamentals of machine / deep learning. In 2024, the class evolved to have more of a focus on the deep learning aspects of the classa and it's application. The course will continue to evolve with time. However, running statistical tests on a datasample, or generating a sample of data based off of a known distribution will continue to be prelavent in both the courses and research.\n",
    "\n",
    "This notebook discusses\n",
    "* How to generate a distribution of data\n",
    "* How to perform basic statistical tests on data\n",
    "* How to convert between score and probability for some distributions # WORK IN PROGRESS\n",
    "\n",
    "There are multiple different python packages out there that can generate distributions and perform statistical tests. Here, we will focus primarily on two well known packages: `numpy` for data generation, and `scipy.stats` for statistical testing.\n",
    "\n",
    "A basic `matplotlib.pyplot` will be used for visualization, `math` will be used for accompanied calculations, and `random` will be used for comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "from scipy import stats\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Generating data distributions with `numpy`\n",
    "\n",
    "`np`'s module `random` (i.e. `np.random`) contains pre-defined generators for different data samples. Documentation for `np.random` can be found [here](https://numpy.org/doc/1.16/reference/routines.random.html).\n",
    "\n",
    "Where as the python inherent package `random` generates a single value, `np.random` is extremely efficient at generating a sample of data`.\n",
    "\n",
    "The most basic and default generator is a uniform distribution.\n",
    "\n",
    "functions `np.random` and `np.rand` both sample a uniform distribution, similar to `random.random`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_np_value = np.random.random()\n",
    "random_rand_value = random.random()\n",
    "print(f'Random value from numpy: {random_np_value}')\n",
    "print(f'Random value from random: {random_rand_value}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted 10 random values from that distribution, we could do so with `random` and a for loop like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "for _ in range(10):\n",
    "    sample.append(random.random())\n",
    "print(f'Random sample of data from a for loop: {sample}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you could use `np.random.random` with a defined *size* for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.random(size = 10)\n",
    "\n",
    "print(f'Random sample of data from numpy: {sample}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interested in lots of random numbers or a distribution of data, a package like `numpy` enables efficient generation.\n",
    "\n",
    "`numpy.random` is home to many different data distribution generators with a full list found [here](https://numpy.org/doc/1.16/reference/routines.random.html#distributions) within the documentation. Below are some key examples of distributions:\n",
    "\n",
    "---\n",
    "### Guassian / Normal Distribution\n",
    "\n",
    "A guassian or normal distribution is defined with respect to the distribution average ($\\mu$, mu) and standard deviation ($\\sigma$, sigma). Following `numpy.random`, those are defined as the location (`loc`) and scale (`scale`) as follows.\n",
    "\n",
    "```python\n",
    "np.random.normal(loc = mu, scale = sigma)\n",
    "```\n",
    "\n",
    "Running that by itself will return a single value sampled from that distribution. If you also define the argument `size` you can generate multiple values. Below, we visualize 100,000 samples of a normal distribution from numpy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 10\n",
    "sigma = 5\n",
    "\n",
    "# Generate a normal distribution of data with location / average = mu & size / standard deviation = sigma\n",
    "normal_dist = np.random.normal(loc=mu, scale=sigma, size=100000)\n",
    "\n",
    "# Calculate a histogram of the data sample for the sake of visualization\n",
    "histogram, bin_edges = np.histogram(normal_dist, bins=100, range = [mu - (sigma * 5), mu + (sigma * 5)], density=True)\n",
    "bin_centers = [(bin_edges[i] + bin_edges[i + 1]) / 2 for i in range(len(bin_edges) - 1)]\n",
    "\n",
    "# Calculate a theoretical distribution based off of the same mu % sigma\n",
    "x = np.asarray(bin_centers)\n",
    "y = (1 / (sigma * m.sqrt(2 * m.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(bin_centers, histogram, label='Sample')\n",
    "ax.plot(x, y, ls='--', c='r', label='Theoretical')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title(f'Normal Distribution Plot (' + r'$\\rm\\mu$' + f' = {mu}, ' + r'$\\rm\\sigma$' + f' = {sigma})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Running statistical tests using scipy\n",
    "\n",
    "There are many different statistical tests to examine differences between datasets that you will come across in class and through research. `scipy`'s module `stats` contains many different useful functions that can aid in your analysis!\n",
    "\n",
    "Here we will use a combination of `np.random` to generate distributions of data and `scipy.stats` to test for their significance.\n",
    "\n",
    "The output of a `scipy.stats` test typically contain atleast two elements, the statistic first and the associated p-value second. Other descriptors may be output as well depending on the t-test given.\n",
    "\n",
    "`scipy.stats` has a ton of different functions that can be found [here](https://docs.scipy.org/doc/scipy/reference/stats.html).\n",
    "\n",
    "---\n",
    "### Paired T-Test\n",
    "\n",
    "A paired t-test examines differences between means of a distribution where the assumption is that each distribution follows a normal or gaussian distribution.\n",
    "\n",
    "There are implentations for both independent or related datasets:\n",
    "\n",
    "```python\n",
    "independent_test = stats.ttest_ind( ... )\n",
    "related_test = stats.ttest_rel( ... )\n",
    "```\n",
    "\n",
    "Typically, the t-test is performed with \"two-tails\". Here, the argument `alternative` is used to define the type of test that you are running. Default is `\"two-sided\"`, however there are options for `\"less\"` and `\"greater\"`.\n",
    "\n",
    "Here, the output is a `TtestResult` that can be treated as a `list` of the relavent elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two groups of n=100 data distributions randomly generated from gaussian distributions\n",
    "# using different mu and sigmas.\n",
    "group_A = np.random.normal(loc = 100, scale = 10, size = 100)\n",
    "group_B = np.random.normal(loc = 80, scale = 5, size = 100)\n",
    "\n",
    "# Running the t-test and gathering the direct output\n",
    "out = stats.ttest_ind(group_A, group_B)\n",
    "print(f'The output of the stats.ttest_ind is: {out}\\n')\n",
    "\n",
    "# Running the t-test and gathering the specific outputs\n",
    "test_statistic, p_value = stats.ttest_ind(group_A, group_B)\n",
    "print(f'The test statistic is: {test_statistic}')\n",
    "print(f'The p value is: {p_value}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Converting between test statistics and p-value\n",
    "\n",
    "A common problem in statistical tests is trying to match test statistic with an associated p-value and vise versa. There are classic look up tables that can be used to get a rough calculation and multiple websites that offer this functionality. Of course, this can also be accomplished in python.\n",
    "\n",
    "The `scipy.stats` package contains distribution samplers (used above), however it also includes the theoretical distribution and can be used to convert test statistic and p-value.\n",
    "\n",
    "A guassian / normal distribution is contained by `scipy.stats.norm`. This distribution contains the property `.ppf` (percent point function) as well as `.cdf` (Cumulative distribution function).\n",
    "\n",
    "The percent point function takes in the lower tail `q`, or p-value, and returns the test statistic associated with it.\n",
    "\n",
    "The cumulative distribution function does the opposite, taking in the test statistic and returns the associated p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = 0.99\n",
    "z_score = stats.norm.ppf(q = p_value)\n",
    "\n",
    "print(f'Considering a Guassian distribution...')\n",
    "print(f'For given p-value {p_value}, the z_score is: {z_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score = 0.86\n",
    "p_value = stats.norm.cdf(z_score)\n",
    "\n",
    "print(f'Considering a Guassian distribution...')\n",
    "print(f'For given z_score {z_score}, the p-value is {p_value}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions work for other distributions as well. When working with specific distributions, make sure to check the documentation to ensure all the required arguments are provided. For instance, the $\\rm\\chi^2$ distribution requires the degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = 0.95\n",
    "df = 3\n",
    "chi2_value = stats.chi2.ppf(q=p_value, df=df)\n",
    "\n",
    "print(f'Considering a chi squared distribution with {df} degrees of freedom')\n",
    "print(f'For given p-value of {p_value}, the chi-squared value is {chi2_value}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Additional Examples:\n",
    "\n",
    "## More examples of data distributions\n",
    "\n",
    "### Binomial or Bernoulli Distribution\n",
    "\n",
    "A descrete distribution of choises.\n",
    "\n",
    "Note, a single choice (n=1, it either is or isn't) is a Bernoulli distribution. However, as you increase the number of choices, you get a Binomial.\n",
    "\n",
    "`np.random.binomial` covers both use cases through defining the hyper parameters `n` for number of choices and `p` for probability of outcome.\n",
    "\n",
    "```python\n",
    "np.random.binomial(n = n, p = p)\n",
    "```\n",
    "\n",
    "n = 1, Bernoulli distribution.\n",
    "\n",
    "i.e. a coin flip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is n = 1 choices (Heads or Tail) where each option has 50% chance of occuring\n",
    "n = 1\n",
    "p = 0.5\n",
    "\n",
    "# Calcualte a binomial distribution of 1000 coin flips\n",
    "binomial_dist = np.random.binomial(n=n, p=p, size=1000)\n",
    "\n",
    "# Calculate a histogram of the data sample for the sake of visualization\n",
    "histogram, bin_edges = np.histogram(binomial_dist, bins=n+1, range = [0, n+1], density=True)\n",
    "bin_centers = [(bin_edges[i] + bin_edges[i + 1]) / 2 for i in range(len(bin_edges) - 1)]\n",
    "\n",
    "# Calculate a theoretical distribution based off of the same mu % sigma\n",
    "x = np.asarray(bin_centers)\n",
    "y = np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(bin_centers, histogram, label='Sample')\n",
    "# ax.plot(x, y, ls='--', c='r', label='Theoretical')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title(f'Binomial Distribution Plot (n = {n}, p = {p})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = 10, Binomial Distribution with a probability shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is n = 1 choices (Heads or Tail) where each option has 50% chance of occuring\n",
    "n = 10\n",
    "p = 0.2\n",
    "\n",
    "# Calcualte a binomial distribution of 1000 coin flips\n",
    "binomial_dist = np.random.binomial(n=n, p=p, size=1000)\n",
    "\n",
    "# Calculate a histogram of the data sample for the sake of visualization\n",
    "histogram, bin_edges = np.histogram(binomial_dist, bins=n+1, range = [0, n+1], density=True)\n",
    "bin_centers = [(bin_edges[i] + bin_edges[i + 1]) / 2 for i in range(len(bin_edges) - 1)]\n",
    "\n",
    "# Calculate a theoretical distribution based off of the same mu % sigma\n",
    "x = np.asarray(bin_centers)\n",
    "y = np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(bin_centers, histogram, label='Sample')\n",
    "# ax.plot(x, y, ls='--', c='r', label='Theoretical')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title(f'Binomial Distribution Plot (n = {n}, p = {p})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Distribution\n",
    "\n",
    "A continous distribution based off of the degrees of freedom (`df`) in an experiment.\n",
    "\n",
    "```python\n",
    "np.random.chisquare(df = df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 3\n",
    "\n",
    "# Generate a Chi Squared distribution with df degrees of freedom\n",
    "chi_squared_distribution = np.random.chisquare(df=df, size=100000)\n",
    "\n",
    "# Calculate a histogram of the data sample for the sake of visualization\n",
    "histogram, bin_edges = np.histogram(chi_squared_distribution, bins=100, range = [0, 15], density=True)\n",
    "bin_centers = [(bin_edges[i] + bin_edges[i + 1]) / 2 for i in range(len(bin_edges) - 1)]\n",
    "\n",
    "# Calculate a theoretical distribution based off of the same mu % sigma\n",
    "x = np.asarray(bin_centers)\n",
    "y = (x ** ((df/2) - 1) * np.exp(-x / 2)) / (2 ** (df / 2) * m.gamma(df / 2))\n",
    "# y = (1 / (sigma * m.sqrt(2 * m.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(bin_centers, histogram, label='Sample')\n",
    "ax.plot(x, y, ls='--', c='r', label='Theoretical')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title(r'$\\rm\\chi^2$' + f' Distribution Plot (df = {df})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Statistical tests\n",
    "\n",
    "### Wilcoxon Sign-Rank Test\n",
    "\n",
    "The paired T-Test is a staple for determining statistical significance. However, the assumption the dataset follows a normal / gaussian distribution must be met and have enough data samples.\n",
    "\n",
    "When a low number of data samples is present, a Wilcoxon Sign-Rank test is a strong alternative in determnining differences between sample *distributions*. Here the assumption is you have equal number of data points in both samples.\n",
    "\n",
    "Note: Wilcoxon tests *distirbution* differences not *mean* differences. \n",
    "\n",
    "Example 1: Two completely different distributions but close in proximity\n",
    "\n",
    "Re-run the cell multiple times and see how often the p value falls below 0.05 (significant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1:\n",
    "# Group A is following a normal distribution\n",
    "# Group B is following a Chi-Squared Distribution\n",
    "\n",
    "group_A = np.random.normal(loc = 5, scale = 2, size = 10)\n",
    "group_B = np.random.chisquare(df = 2, size = 10)\n",
    "\n",
    "# Running the Wilcoxon Sign-Rank test and gathering the specific outputs\n",
    "print(f'Output of Wilcoxon Sign-Rank test:')\n",
    "test_statistic, p_value = stats.wilcoxon(group_A, group_B)\n",
    "print(f'The test statistic is: {test_statistic}')\n",
    "print(f'The p value is: {p_value}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Two very similar Gaussian distributions\n",
    "\n",
    "Re-run the cell multiple times and see how often the p value falls below 0.05 (significant). Note how p-values differ between the Wilcoxon sign-rank test and the equivalent paired t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2:\n",
    "# Group A is following a normal distribution\n",
    "# Group B is following a similar normal distribution\n",
    "\n",
    "group_A = np.random.normal(loc = 5, scale = 2, size = 5)\n",
    "group_B = np.random.normal(loc = 2, scale = 3, size = 5)\n",
    "\n",
    "# Running the Wilcoxon Sign-Rank test and gathering the specific outputs\n",
    "print(f'Output of the Wilcoxon Sign-Rank test:')\n",
    "test_statistic, p_value = stats.wilcoxon(group_A, group_B)\n",
    "print(f'The test statistic is: {test_statistic}')\n",
    "print(f'The p value is: {p_value}\\n')\n",
    "\n",
    "\n",
    "# Running an equivalent paired t-test to see how the p-values differe.\n",
    "print(f'Output of an equivalent t-test:')\n",
    "test_statistic, p_value = stats.ttest_ind(group_A, group_B)\n",
    "print(f'The test statistic is: {test_statistic}')\n",
    "print(f'The p value is: {p_value}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Mann-Whitney U-Test\n",
    "\n",
    "A Wilcoxon Sign-Rank test gets the job done when you are experimenting with a small n cohort. However, the test still assumes you have an equal number of data in both cohorts. In the events where that *doesn't* occur, you can use a Mann-Whitney U-Test.\n",
    "\n",
    "A Mann-Whiteny U-Test does a very similar thing as the Wilcoxon Sign-Rank test. However, it can handle differences in datasize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group A is following a normal distribution with n = 10\n",
    "# Group B is following a Chi-Squared Distribution with n = 7\n",
    "\n",
    "group_A = np.random.normal(loc = 5, scale = 2, size = 10)\n",
    "group_B = np.random.chisquare(df = 3, size = 7)\n",
    "\n",
    "# Running the Mann-Whiteny U-Test and gathering the specific outputs\n",
    "print(f'Output of the Mann-Whiteny U-Test:')\n",
    "test_statistic, p_value = stats.mannwhitneyu(group_A, group_B)\n",
    "print(f'The test statistic is: {test_statistic}')\n",
    "print(f'The p value is: {p_value}\\n')\n",
    "\n",
    "print(f'Output of an equivalent Wilcoxon Sign-Rank test:')\n",
    "test_statistic, p_value = stats.wilcoxon(group_A, group_B)\n",
    "print(f'The test statistic is: {test_statistic}')\n",
    "print(f'The p value is: {p_value}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the Mann-Whitney U-test can handle uneven datasets. However, the Wilcoxon signed rank test requires equal samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "252264a34d161799bd39aad204a51967c68e9f535c5b1dfddf376bf7327a5eda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
